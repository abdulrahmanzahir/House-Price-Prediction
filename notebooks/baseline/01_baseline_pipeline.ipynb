{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Ames Housing Pipeline\n",
    "**Adapted from** Aarthi93’s “End-to-End ML pipeline”  \n",
    "**Original source & license:**  \n",
    "https://www.kaggle.com/code/aarthi93/end-to-end-ml-pipeline  \n",
    "Released under the Apache 2.0 License: https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T16:43:30.741298Z",
     "iopub.status.busy": "2025-04-10T16:43:30.740991Z",
     "iopub.status.idle": "2025-04-10T16:43:33.226233Z",
     "shell.execute_reply": "2025-04-10T16:43:33.22522Z",
     "shell.execute_reply.started": "2025-04-10T16:43:30.741267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:44:06.353379Z",
     "iopub.status.busy": "2025-04-10T16:44:06.353019Z",
     "iopub.status.idle": "2025-04-10T16:44:08.366611Z",
     "shell.execute_reply": "2025-04-10T16:44:08.365286Z",
     "shell.execute_reply.started": "2025-04-10T16:44:06.353354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 1. SETUP\n",
    "# ======================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T16:55:03.154828Z",
     "iopub.status.busy": "2025-04-10T16:55:03.154454Z",
     "iopub.status.idle": "2025-04-10T16:55:03.216858Z",
     "shell.execute_reply": "2025-04-10T16:55:03.215675Z",
     "shell.execute_reply.started": "2025-04-10T16:55:03.154806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data from Kaggle input directory\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the Ames housing dataset from Kaggle's input directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw housing data\n",
    "    \"\"\"\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:08:22.407217Z",
     "iopub.status.busy": "2025-04-10T17:08:22.406843Z",
     "iopub.status.idle": "2025-04-10T17:08:22.483165Z",
     "shell.execute_reply": "2025-04-10T17:08:22.481969Z",
     "shell.execute_reply.started": "2025-04-10T17:08:22.407196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. DATA LOADING\n",
    "# ======================\n",
    "def load_data():\n",
    "    \"\"\"Loads and prints initial data\"\"\"\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            if 'Ames' in filename:\n",
    "                df = pd.read_csv(os.path.join(dirname, filename))\n",
    "                print(\"\\n=== RAW DATA ===\")\n",
    "                print(f\"Shape: {df.shape}\")\n",
    "                print(\"First 3 rows:\")\n",
    "                display(df.head(3))\n",
    "                print(\"\\nData types:\")\n",
    "                print(df.dtypes.value_counts())\n",
    "                return df\n",
    "    raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "raw_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:08:46.078483Z",
     "iopub.status.busy": "2025-04-10T17:08:46.078186Z",
     "iopub.status.idle": "2025-04-10T17:08:46.152698Z",
     "shell.execute_reply": "2025-04-10T17:08:46.151511Z",
     "shell.execute_reply.started": "2025-04-10T17:08:46.078462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. DATA CLEANING\n",
    "# ======================\n",
    "def clean_data(df):\n",
    "    \"\"\"Cleans data with printouts\"\"\"\n",
    "    print(\"\\n=== CLEANING DATA ===\")\n",
    "    \n",
    "    # Columns to drop with reasons\n",
    "    cols_to_drop = {\n",
    "        'Order': 'Index column',\n",
    "        'PID': 'Property ID',\n",
    "        'Alley': '93% missing',\n",
    "        'Pool QC': '99% missing', \n",
    "        'Fence': '80% missing',\n",
    "        'Misc Feature': '96% missing',\n",
    "        'Garage Yr Blt': 'Redundant with Year Built',\n",
    "        'Mo Sold': 'Potential leakage',\n",
    "        'Yr Sold': 'Potential leakage'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDropping columns: {cols_to_drop.keys()}\")\n",
    "    df_clean = df.drop(columns=cols_to_drop.keys())\n",
    "    \n",
    "    # Before filling missing\n",
    "    print(\"\\nMissing values BEFORE filling:\")\n",
    "    print(df_clean.isna().sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Fill missing\n",
    "    df_filled = df_clean.copy()\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_filled[col] = df_clean[col].fillna('None')\n",
    "        else:\n",
    "            df_filled[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # After filling missing\n",
    "    print(\"\\nMissing values AFTER filling:\")\n",
    "    print(df_filled.isna().sum().sum(), \"total missing values remaining\")\n",
    "    \n",
    "    print(\"\\nCleaned data shape:\", df_filled.shape)\n",
    "    print(\"\\nSample of cleaned data:\")\n",
    "    display(df_filled.head(3))\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "cleaned_data = clean_data(raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:09:15.447745Z",
     "iopub.status.busy": "2025-04-10T17:09:15.447309Z",
     "iopub.status.idle": "2025-04-10T17:09:15.482384Z",
     "shell.execute_reply": "2025-04-10T17:09:15.481501Z",
     "shell.execute_reply.started": "2025-04-10T17:09:15.44772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. FEATURE ENGINEERING \n",
    "# ======================\n",
    "def engineer_features(df):\n",
    "    \"\"\"Engineers features with printouts\"\"\"\n",
    "    print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "    CURRENT_YEAR = 2025\n",
    "    \n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # New features\n",
    "    df_eng['House_Age'] = CURRENT_YEAR - df['Year Built']\n",
    "    df_eng['Years_Since_Remodel'] = CURRENT_YEAR - df['Year Remod/Add']\n",
    "    df_eng['Total_SF'] = df['Total Bsmt SF'] + df['1st Flr SF'] + df['2nd Flr SF']\n",
    "    df_eng['Total_Bathrooms'] = (df['Full Bath'] + 0.5*df['Half Bath'] + \n",
    "                                df['Bsmt Full Bath'] + 0.5*df['Bsmt Half Bath'])\n",
    "    df_eng['Has_Pool'] = (df['Pool Area'] > 0).astype(int)\n",
    "    \n",
    "    # Columns to drop\n",
    "    cols_to_drop = [\n",
    "        'Year Built', 'Year Remod/Add', 'Total Bsmt SF',\n",
    "        '1st Flr SF', '2nd Flr SF', 'Full Bath', 'Half Bath',\n",
    "        'Bsmt Full Bath', 'Bsmt Half Bath', 'Pool Area'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nAdded new features:\")\n",
    "    print([col for col in df_eng.columns if col not in df.columns])\n",
    "    \n",
    "    print(\"\\nDropping original columns:\", cols_to_drop)\n",
    "    df_final = df_eng.drop(columns=cols_to_drop)\n",
    "    \n",
    "    print(\"\\nEngineered data shape:\", df_final.shape)\n",
    "    print(\"\\nSample with new features:\")\n",
    "    display(df_final.head(3))\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "engineered_data = engineer_features(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:00:38.905Z",
     "iopub.status.busy": "2025-04-10T17:00:38.904676Z",
     "iopub.status.idle": "2025-04-10T17:00:38.922035Z",
     "shell.execute_reply": "2025-04-10T17:00:38.921097Z",
     "shell.execute_reply.started": "2025-04-10T17:00:38.904981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5. DATA PREPROCESSING\n",
    "# ======================\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepares final data with printouts\"\"\"\n",
    "    print(\"\\n=== FINAL DATA PREP ===\")\n",
    "    \n",
    "    X = df.drop('SalePrice', axis=1)\n",
    "    y = df['SalePrice']\n",
    "    \n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = X.select_dtypes(include='object').columns\n",
    "    \n",
    "    print(f\"\\nNumeric features ({len(numeric_cols)}):\")\n",
    "    print(numeric_cols.tolist())\n",
    "    \n",
    "    print(f\"\\nCategorical features ({len(categorical_cols)}):\")\n",
    "    print(categorical_cols.tolist())\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTrain/test split sizes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, numeric_cols, categorical_cols\n",
    "\n",
    "X_train, X_test, y_train, y_test, numeric_cols, categorical_cols = prepare_data(engineered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:10:20.524472Z",
     "iopub.status.busy": "2025-04-10T17:10:20.523382Z",
     "iopub.status.idle": "2025-04-10T17:10:23.783841Z",
     "shell.execute_reply": "2025-04-10T17:10:23.782676Z",
     "shell.execute_reply.started": "2025-04-10T17:10:20.524436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. MODEL BUILDING & INSPECTION\n",
    "# ======================\n",
    "print(\"\\n=== MODEL BUILDING ===\")\n",
    "\n",
    "# Build pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=30,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Print full model structure\n",
    "print(\"\\n=== FULL MODEL STRUCTURE ===\")\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')  # Enable visual display\n",
    "display(model)  # This will show the complete pipeline diagram\n",
    "\n",
    "# Alternative text representation\n",
    "print(\"\\n=== DETAILED MODEL PARAMETERS ===\")\n",
    "from pprint import pprint\n",
    "print(\"\\nPreprocessor configuration:\")\n",
    "pprint(preprocessor.get_params())\n",
    "\n",
    "print(\"\\nRegressor configuration:\")\n",
    "pprint(model.named_steps['regressor'].get_params())\n",
    "\n",
    "# Print feature names after transformation\n",
    "print(\"\\n=== TRANSFORMED FEATURE NAMES ===\")\n",
    "model.fit(X_train, y_train)  # Need to fit first to get feature names\n",
    "\n",
    "# Get feature names from one-hot encoding\n",
    "encoder = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder']\n",
    "cat_features = encoder.get_feature_names_out(categorical_cols)\n",
    "all_features = np.concatenate([numeric_cols, cat_features])\n",
    "\n",
    "print(f\"\\nTotal features after preprocessing: {len(all_features)}\")\n",
    "print(\"\\nFirst 20 feature names:\")\n",
    "print(all_features[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:13:32.853564Z",
     "iopub.status.busy": "2025-04-10T17:13:32.852568Z",
     "iopub.status.idle": "2025-04-10T17:14:10.424487Z",
     "shell.execute_reply": "2025-04-10T17:14:10.423197Z",
     "shell.execute_reply.started": "2025-04-10T17:13:32.853507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 7. TRAINING & EVALUATION \n",
    "# ======================\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n=== MODEL TRAINING ===\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "print(\"\\n=== EVALUATION ===\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 1. Basic Metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== BASIC METRICS ===\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"MAE: ${mae:,.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# 2. Prediction Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.show()\n",
    "\n",
    "# 3. Error Distribution\n",
    "errors = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(errors, kde=True)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.xlabel('Prediction Errors')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature Importance (Detailed)\n",
    "importances = model.named_steps['regressor'].feature_importances_\n",
    "feature_names = np.concatenate([\n",
    "    numeric_cols,\n",
    "    model.named_steps['preprocessor']\n",
    "    .named_transformers_['cat']\n",
    "    .named_steps['encoder']\n",
    "    .get_feature_names_out(categorical_cols)\n",
    "])\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "top_features = feature_importance.sort_values('Importance', ascending=False).head(20)\n",
    "\n",
    "print(\"\\n=== TOP 20 FEATURES ===\")\n",
    "display(top_features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Residual Analysis\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Analysis')\n",
    "plt.show()\n",
    "\n",
    "# 6. Model Learning Curve (Optional)\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    model, X_train, y_train, cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "train_scores_mean = np.sqrt(-train_scores.mean(axis=1))\n",
    "test_scores_mean = np.sqrt(-test_scores.mean(axis=1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Validation')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3428617,
     "sourceId": 5981646,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
